/*************************************************************************************************

Welcome to Baml! To use this generated code, please run one of the following:

$ npm install @boundaryml/baml
$ yarn add @boundaryml/baml
$ pnpm add @boundaryml/baml

*************************************************************************************************/

// This file was generated by BAML: please do not edit it. Instead, edit the
// BAML files and re-generate this code using: baml-cli generate
// You can install baml-cli with:
//  $ npm install @boundaryml/baml
//
/* eslint-disable */
// tslint:disable
// @ts-nocheck
// biome-ignore format: autogenerated code

const fileMap = {
  
  "chat.baml": "class Message {\r\n  role \"user\" | \"assistant\" | \"system\"\r\n  content string\r\n}\r\n\r\nfunction Chat(messages: Message[]) -> string {\r\n  client \"Locals\"\r\n  prompt #\"\r\n    {% for m in messages %}\r\n    {{ _.role(m.role)}}\r\n    {{m.content}}\r\n    {% endfor %}\r\n  \"#\r\n}\r\n\r\ntest TestName {\r\n  functions [Chat]\r\n  args {\r\n    messages [\r\n      {\r\n        role \"user\"\r\n        content \"Hello!\"\r\n      },\r\n      {\r\n        role \"assistant\"\r\n        content \"Hello, how can I help you?\"\r\n      },\r\n      {\r\n        role \"user\"\r\n        content \"help me understand Chobani's success\"\r\n      }\r\n    ]\r\n  }\r\n}\r\n",
  "clients.baml": "client<llm> LMS {\r\n  provider openai-generic\r\n  options {\r\n    base_url \"http://localhost:1234/v1\"\r\n    model \"llamqwen3-4b-instruct-2507a4\"\r\n    default_role \"user\" // Most local models prefer the user role\r\n    // No API key needed for local Ollama\r\n  }\r\n}\r\n\r\nclient<llm> LlamaCPP {\r\n  provider openai-generic\r\n  options {\r\n    base_url \"http://localhost:8080/v1\"\r\n    model \"llamqwen3-4b-instruct-2507\"\r\n    default_role \"user\" // Most local models prefer the user role\r\n    // No API key needed for local Ollama\r\n  }\r\n}\r\n\r\n// https://docs.boundaryml.com/docs/snippets/clients/fallback\r\nclient<llm> Locals {\r\n  provider fallback\r\n  options {\r\n    // This will try the clients in order until one succeeds\r\n    strategy [LMS, LlamaCPP]\r\n  }\r\n}\r\n",
  "generators.baml": "// This helps use auto generate libraries you can use in the language of\n// your choice. You can have multiple generators if you use multiple languages.\n// Just ensure that the output_dir is different for each generator.\ngenerator target {\n    // Valid values: \"python/pydantic\", \"typescript\", \"ruby/sorbet\", \"rest/openapi\"\n    output_type \"typescript\"\n\n    // Where the generated code will be saved (relative to baml_src/)\n    output_dir \"../\"\n\n    // The version of the BAML package you have installed (e.g. same version as your baml-py or @boundaryml/baml).\n    // The BAML VSCode extension version should also match this version.\n    version \"0.211.2\"\n\n    // Valid values: \"sync\", \"async\"\n    // This controls what `b.FunctionName()` will be (sync or async).\n    default_client_mode async\n}\n",
}
export const getBamlFiles = () => {
    return fileMap;
}